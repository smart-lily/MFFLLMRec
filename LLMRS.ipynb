{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qml4KEOYTMeA",
        "outputId": "c4fc3617-7604-4674-e071-72ec72b1cf23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TALLRec'...\n",
            "remote: Enumerating objects: 219, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 219 (delta 69), reused 79 (delta 64), pack-reused 128 (from 1)\u001b[K\n",
            "Receiving objects: 100% (219/219), 5.77 MiB | 8.06 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/SAI990323/TALLRec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UTT7l2_NBPbB",
        "outputId": "872c0b67-5fbf-473c-859a-b3a34c3cc0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft==0.3.0\n",
            "  Downloading peft-0.3.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (4.46.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.3)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->peft==0.3.0) (0.4.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (0.20.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.9.0\n",
            "    Uninstalling peft-0.9.0:\n",
            "      Successfully uninstalled peft-0.9.0\n",
            "Successfully installed peft-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "peft"
                ]
              },
              "id": "f9e89404165a47cb92fa30ab18b4364b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install peft==0.3.0 fire datasets bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN3nZKEsUoE1"
      },
      "outputs": [],
      "source": [
        "! chmod 777 ./TALLRec/shell/instruct_7B.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdyJFd2uU6EF",
        "outputId": "b5608dd1-f844-42d4-f8f6-cb3505792b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TALLRec\n"
          ]
        }
      ],
      "source": [
        "%cd TALLRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA6u39ENTkff",
        "outputId": "91ddf4a2-6a78-4e73-8ca9-f412c5ec05e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 42\n",
            "lr: 1e-4, dropout: 0.05 , seed: 42, sample: 64\n",
            "2024-11-14 11:38:56.743624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 11:38:56.777564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 11:38:56.787481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 11:38:56.810469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 11:38:58.497570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Training Alpaca-LoRA model with params:\n",
            "base_model: baffo32/decapoda-research-llama-7B-hf\n",
            "train_data_path: ./data/movie/test.json\n",
            "val_data_path: ./data/movie/valid.json\n",
            "sample: 64\n",
            "seed: 42\n",
            "output_dir: output_42_64\n",
            "batch_size: 16\n",
            "micro_batch_size: 16\n",
            "num_epochs: 10\n",
            "learning_rate: 0.0001\n",
            "cutoff_len: 512\n",
            "lora_r: 8\n",
            "lora_alpha: 16\n",
            "lora_dropout: 0.05\n",
            "lora_target_modules: ['q_proj', 'v_proj']\n",
            "train_on_inputs: True\n",
            "group_by_length: True\n",
            "wandb_project: \n",
            "wandb_run_name: \n",
            "wandb_watch: \n",
            "wandb_log_model: \n",
            "resume_from_checkpoint: model\n",
            "\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:568: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 33/33 [01:10<00:00,  2.15s/it]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:145: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "Checkpoint model/adapter_model.bin not found\n",
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "  0% 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 1.9416, 'grad_norm': 1.6838654279708862, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
            " 25% 10/40 [04:04<12:14, 24.48s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:01<01:48,  1.13it/s]\u001b[A\n",
            "  2% 3/125 [00:03<02:33,  1.26s/it]\u001b[A\n",
            "  3% 4/125 [00:05<02:53,  1.44s/it]\u001b[A\n",
            "  4% 5/125 [00:07<03:04,  1.54s/it]\u001b[A\n",
            "  5% 6/125 [00:08<03:11,  1.61s/it]\u001b[A\n",
            "  6% 7/125 [00:10<03:13,  1.64s/it]\u001b[A\n",
            "  6% 8/125 [00:12<03:15,  1.67s/it]\u001b[A\n",
            "  7% 9/125 [00:13<03:16,  1.69s/it]\u001b[A\n",
            "  8% 10/125 [00:15<03:14,  1.70s/it]\u001b[A\n",
            "  9% 11/125 [00:17<03:13,  1.70s/it]\u001b[A\n",
            " 10% 12/125 [00:19<03:12,  1.70s/it]\u001b[A\n",
            " 10% 13/125 [00:20<03:10,  1.70s/it]\u001b[A\n",
            " 11% 14/125 [00:22<03:09,  1.70s/it]\u001b[A\n",
            " 12% 15/125 [00:24<03:05,  1.68s/it]\u001b[A\n",
            " 13% 16/125 [00:25<03:01,  1.67s/it]\u001b[A\n",
            " 14% 17/125 [00:27<02:58,  1.65s/it]\u001b[A\n",
            " 14% 18/125 [00:28<02:54,  1.63s/it]\u001b[A\n",
            " 15% 19/125 [00:30<02:52,  1.62s/it]\u001b[A\n",
            " 16% 20/125 [00:32<02:49,  1.62s/it]\u001b[A\n",
            " 17% 21/125 [00:33<02:47,  1.61s/it]\u001b[A\n",
            " 18% 22/125 [00:35<02:45,  1.61s/it]\u001b[A\n",
            " 18% 23/125 [00:36<02:44,  1.61s/it]\u001b[A\n",
            " 19% 24/125 [00:38<02:43,  1.62s/it]\u001b[A\n",
            " 20% 25/125 [00:40<02:40,  1.61s/it]\u001b[A\n",
            " 21% 26/125 [00:41<02:37,  1.59s/it]\u001b[A\n",
            " 22% 27/125 [00:43<02:35,  1.58s/it]\u001b[A\n",
            " 22% 28/125 [00:44<02:33,  1.58s/it]\u001b[A\n",
            " 23% 29/125 [00:46<02:30,  1.57s/it]\u001b[A\n",
            " 24% 30/125 [00:47<02:29,  1.57s/it]\u001b[A\n",
            " 25% 31/125 [00:49<02:26,  1.56s/it]\u001b[A\n",
            " 26% 32/125 [00:51<02:36,  1.68s/it]\u001b[A\n",
            " 26% 33/125 [00:53<02:42,  1.76s/it]\u001b[A\n",
            " 27% 34/125 [00:55<02:40,  1.77s/it]\u001b[A\n",
            " 28% 35/125 [00:56<02:39,  1.77s/it]\u001b[A\n",
            " 29% 36/125 [00:58<02:36,  1.76s/it]\u001b[A\n",
            " 30% 37/125 [01:00<02:34,  1.75s/it]\u001b[A\n",
            " 30% 38/125 [01:02<02:31,  1.75s/it]\u001b[A\n",
            " 31% 39/125 [01:03<02:29,  1.74s/it]\u001b[A\n",
            " 32% 40/125 [01:05<02:27,  1.74s/it]\u001b[A\n",
            " 33% 41/125 [01:07<02:25,  1.73s/it]\u001b[A\n",
            " 34% 42/125 [01:09<02:22,  1.72s/it]\u001b[A\n",
            " 34% 43/125 [01:10<02:20,  1.72s/it]\u001b[A\n",
            " 35% 44/125 [01:12<02:18,  1.71s/it]\u001b[A\n",
            " 36% 45/125 [01:14<02:16,  1.71s/it]\u001b[A\n",
            " 37% 46/125 [01:15<02:14,  1.71s/it]\u001b[A\n",
            " 38% 47/125 [01:17<02:11,  1.68s/it]\u001b[A\n",
            " 38% 48/125 [01:19<02:08,  1.67s/it]\u001b[A\n",
            " 39% 49/125 [01:20<02:05,  1.65s/it]\u001b[A\n",
            " 40% 50/125 [01:22<02:02,  1.63s/it]\u001b[A\n",
            " 41% 51/125 [01:23<02:00,  1.62s/it]\u001b[A\n",
            " 42% 52/125 [01:25<01:57,  1.62s/it]\u001b[A\n",
            " 42% 53/125 [01:27<01:55,  1.61s/it]\u001b[A\n",
            " 43% 54/125 [01:28<01:54,  1.61s/it]\u001b[A\n",
            " 44% 55/125 [01:30<01:52,  1.61s/it]\u001b[A\n",
            " 45% 56/125 [01:31<01:50,  1.61s/it]\u001b[A\n",
            " 46% 57/125 [01:33<01:48,  1.60s/it]\u001b[A\n",
            " 46% 58/125 [01:35<01:46,  1.59s/it]\u001b[A\n",
            " 47% 59/125 [01:36<01:44,  1.58s/it]\u001b[A\n",
            " 48% 60/125 [01:38<01:42,  1.57s/it]\u001b[A\n",
            " 49% 61/125 [01:39<01:39,  1.56s/it]\u001b[A\n",
            " 50% 62/125 [01:41<01:37,  1.55s/it]\u001b[A\n",
            " 50% 63/125 [01:43<01:43,  1.67s/it]\u001b[A\n",
            " 51% 64/125 [01:45<01:44,  1.72s/it]\u001b[A\n",
            " 52% 65/125 [01:46<01:44,  1.74s/it]\u001b[A\n",
            " 53% 66/125 [01:48<01:42,  1.74s/it]\u001b[A\n",
            " 54% 67/125 [01:50<01:40,  1.74s/it]\u001b[A\n",
            " 54% 68/125 [01:52<01:39,  1.74s/it]\u001b[A\n",
            " 55% 69/125 [01:53<01:37,  1.74s/it]\u001b[A\n",
            " 56% 70/125 [01:55<01:35,  1.74s/it]\u001b[A\n",
            " 57% 71/125 [01:57<01:33,  1.73s/it]\u001b[A\n",
            " 58% 72/125 [01:58<01:31,  1.73s/it]\u001b[A\n",
            " 58% 73/125 [02:00<01:29,  1.72s/it]\u001b[A\n",
            " 59% 74/125 [02:02<01:27,  1.72s/it]\u001b[A\n",
            " 60% 75/125 [02:04<01:25,  1.71s/it]\u001b[A\n",
            " 61% 76/125 [02:05<01:23,  1.71s/it]\u001b[A\n",
            " 62% 77/125 [02:07<01:21,  1.71s/it]\u001b[A\n",
            " 62% 78/125 [02:09<01:20,  1.71s/it]\u001b[A\n",
            " 63% 79/125 [02:10<01:17,  1.68s/it]\u001b[A\n",
            " 64% 80/125 [02:12<01:14,  1.66s/it]\u001b[A\n",
            " 65% 81/125 [02:14<01:12,  1.64s/it]\u001b[A\n",
            " 66% 82/125 [02:15<01:10,  1.63s/it]\u001b[A\n",
            " 66% 83/125 [02:17<01:08,  1.62s/it]\u001b[A\n",
            " 67% 84/125 [02:18<01:06,  1.61s/it]\u001b[A\n",
            " 68% 85/125 [02:20<01:04,  1.61s/it]\u001b[A\n",
            " 69% 86/125 [02:22<01:02,  1.61s/it]\u001b[A\n",
            " 70% 87/125 [02:23<01:00,  1.60s/it]\u001b[A\n",
            " 70% 88/125 [02:25<00:59,  1.60s/it]\u001b[A\n",
            " 71% 89/125 [02:26<00:57,  1.59s/it]\u001b[A\n",
            " 72% 90/125 [02:28<00:55,  1.58s/it]\u001b[A\n",
            " 73% 91/125 [02:29<00:53,  1.57s/it]\u001b[A\n",
            " 74% 92/125 [02:31<00:51,  1.56s/it]\u001b[A\n",
            " 74% 93/125 [02:32<00:49,  1.55s/it]\u001b[A\n",
            " 75% 94/125 [02:34<00:51,  1.67s/it]\u001b[A\n",
            " 76% 95/125 [02:36<00:51,  1.70s/it]\u001b[A\n",
            " 77% 96/125 [02:38<00:49,  1.72s/it]\u001b[A\n",
            " 78% 97/125 [02:40<00:48,  1.73s/it]\u001b[A\n",
            " 78% 98/125 [02:41<00:46,  1.73s/it]\u001b[A\n",
            " 79% 99/125 [02:43<00:45,  1.73s/it]\u001b[A\n",
            " 80% 100/125 [02:45<00:43,  1.73s/it]\u001b[A\n",
            " 81% 101/125 [02:47<00:41,  1.73s/it]\u001b[A\n",
            " 82% 102/125 [02:48<00:39,  1.72s/it]\u001b[A\n",
            " 82% 103/125 [02:50<00:37,  1.72s/it]\u001b[A\n",
            " 83% 104/125 [02:52<00:36,  1.72s/it]\u001b[A\n",
            " 84% 105/125 [02:53<00:34,  1.72s/it]\u001b[A\n",
            " 85% 106/125 [02:55<00:32,  1.71s/it]\u001b[A\n",
            " 86% 107/125 [02:57<00:30,  1.71s/it]\u001b[A\n",
            " 86% 108/125 [02:59<00:29,  1.71s/it]\u001b[A\n",
            " 87% 109/125 [03:00<00:27,  1.71s/it]\u001b[A\n",
            " 88% 110/125 [03:02<00:25,  1.68s/it]\u001b[A\n",
            " 89% 111/125 [03:03<00:23,  1.66s/it]\u001b[A\n",
            " 90% 112/125 [03:05<00:21,  1.65s/it]\u001b[A\n",
            " 90% 113/125 [03:07<00:19,  1.64s/it]\u001b[A\n",
            " 91% 114/125 [03:08<00:17,  1.63s/it]\u001b[A\n",
            " 92% 115/125 [03:10<00:16,  1.62s/it]\u001b[A\n",
            " 93% 116/125 [03:12<00:14,  1.61s/it]\u001b[A\n",
            " 94% 117/125 [03:13<00:12,  1.60s/it]\u001b[A\n",
            " 94% 118/125 [03:15<00:11,  1.59s/it]\u001b[A\n",
            " 95% 119/125 [03:16<00:09,  1.59s/it]\u001b[A\n",
            " 96% 120/125 [03:18<00:07,  1.59s/it]\u001b[A\n",
            " 97% 121/125 [03:19<00:06,  1.58s/it]\u001b[A\n",
            " 98% 122/125 [03:21<00:04,  1.58s/it]\u001b[A\n",
            " 98% 123/125 [03:22<00:03,  1.56s/it]\u001b[A\n",
            " 99% 124/125 [03:24<00:01,  1.55s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.816799283027649, 'eval_auc': 0.42389662098508685, 'eval_runtime': 208.3964, 'eval_samples_per_second': 4.799, 'eval_steps_per_second': 0.6, 'epoch': 2.5}\n",
            " 25% 10/40 [07:33<12:14, 24.48s/it]\n",
            "100% 125/125 [03:26<00:00,  1.63s/it]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 1.8459, 'grad_norm': 0.5584515333175659, 'learning_rate': 7e-05, 'epoch': 4.0}\n",
            " 50% 20/40 [11:35<08:53, 26.69s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:01<01:48,  1.13it/s]\u001b[A\n",
            "  2% 3/125 [00:03<02:32,  1.25s/it]\u001b[A\n",
            "  3% 4/125 [00:05<02:53,  1.43s/it]\u001b[A\n",
            "  4% 5/125 [00:06<03:04,  1.53s/it]\u001b[A\n",
            "  5% 6/125 [00:08<03:10,  1.60s/it]\u001b[A\n",
            "  6% 7/125 [00:10<03:14,  1.64s/it]\u001b[A\n",
            "  6% 8/125 [00:12<03:15,  1.67s/it]\u001b[A\n",
            "  7% 9/125 [00:13<03:15,  1.69s/it]\u001b[A\n",
            "  8% 10/125 [00:15<03:14,  1.69s/it]\u001b[A\n",
            "  9% 11/125 [00:17<03:13,  1.69s/it]\u001b[A\n",
            " 10% 12/125 [00:19<03:11,  1.70s/it]\u001b[A\n",
            " 10% 13/125 [00:20<03:10,  1.70s/it]\u001b[A\n",
            " 11% 14/125 [00:22<03:08,  1.70s/it]\u001b[A\n",
            " 12% 15/125 [00:24<03:07,  1.70s/it]\u001b[A\n",
            " 13% 16/125 [00:25<03:06,  1.71s/it]\u001b[A\n",
            " 14% 17/125 [00:27<03:04,  1.71s/it]\u001b[A\n",
            " 14% 18/125 [00:29<02:59,  1.67s/it]\u001b[A\n",
            " 15% 19/125 [00:30<02:54,  1.65s/it]\u001b[A\n",
            " 16% 20/125 [00:32<02:51,  1.63s/it]\u001b[A\n",
            " 17% 21/125 [00:33<02:48,  1.62s/it]\u001b[A\n",
            " 18% 22/125 [00:35<02:45,  1.61s/it]\u001b[A\n",
            " 18% 23/125 [00:37<02:44,  1.61s/it]\u001b[A\n",
            " 19% 24/125 [00:38<02:43,  1.61s/it]\u001b[A\n",
            " 20% 25/125 [00:40<02:40,  1.61s/it]\u001b[A\n",
            " 21% 26/125 [00:41<02:37,  1.59s/it]\u001b[A\n",
            " 22% 27/125 [00:43<02:35,  1.58s/it]\u001b[A\n",
            " 22% 28/125 [00:45<02:33,  1.58s/it]\u001b[A\n",
            " 23% 29/125 [00:46<02:30,  1.57s/it]\u001b[A\n",
            " 24% 30/125 [00:48<02:27,  1.56s/it]\u001b[A\n",
            " 25% 31/125 [00:49<02:25,  1.54s/it]\u001b[A\n",
            " 26% 32/125 [00:51<02:35,  1.67s/it]\u001b[A\n",
            " 26% 33/125 [00:53<02:37,  1.72s/it]\u001b[A\n",
            " 27% 34/125 [00:55<02:37,  1.73s/it]\u001b[A\n",
            " 28% 35/125 [00:56<02:36,  1.74s/it]\u001b[A\n",
            " 29% 36/125 [00:58<02:34,  1.74s/it]\u001b[A\n",
            " 30% 37/125 [01:00<02:33,  1.74s/it]\u001b[A\n",
            " 30% 38/125 [01:02<02:31,  1.74s/it]\u001b[A\n",
            " 31% 39/125 [01:03<02:28,  1.73s/it]\u001b[A\n",
            " 32% 40/125 [01:05<02:26,  1.73s/it]\u001b[A\n",
            " 33% 41/125 [01:07<02:24,  1.73s/it]\u001b[A\n",
            " 34% 42/125 [01:09<02:22,  1.72s/it]\u001b[A\n",
            " 34% 43/125 [01:10<02:20,  1.71s/it]\u001b[A\n",
            " 35% 44/125 [01:12<02:18,  1.71s/it]\u001b[A\n",
            " 36% 45/125 [01:14<02:16,  1.71s/it]\u001b[A\n",
            " 37% 46/125 [01:15<02:14,  1.70s/it]\u001b[A\n",
            " 38% 47/125 [01:17<02:13,  1.71s/it]\u001b[A\n",
            " 38% 48/125 [01:19<02:08,  1.67s/it]\u001b[A\n",
            " 39% 49/125 [01:20<02:05,  1.65s/it]\u001b[A\n",
            " 40% 50/125 [01:22<02:02,  1.64s/it]\u001b[A\n",
            " 41% 51/125 [01:23<02:00,  1.62s/it]\u001b[A\n",
            " 42% 52/125 [01:25<01:57,  1.61s/it]\u001b[A\n",
            " 42% 53/125 [01:27<01:55,  1.61s/it]\u001b[A\n",
            " 43% 54/125 [01:28<01:54,  1.61s/it]\u001b[A\n",
            " 44% 55/125 [01:30<01:52,  1.61s/it]\u001b[A\n",
            " 45% 56/125 [01:31<01:51,  1.61s/it]\u001b[A\n",
            " 46% 57/125 [01:33<01:49,  1.62s/it]\u001b[A\n",
            " 46% 58/125 [01:35<01:47,  1.60s/it]\u001b[A\n",
            " 47% 59/125 [01:36<01:44,  1.59s/it]\u001b[A\n",
            " 48% 60/125 [01:38<01:42,  1.58s/it]\u001b[A\n",
            " 49% 61/125 [01:39<01:40,  1.58s/it]\u001b[A\n",
            " 50% 62/125 [01:41<01:38,  1.56s/it]\u001b[A\n",
            " 50% 63/125 [01:43<01:44,  1.68s/it]\u001b[A\n",
            " 51% 64/125 [01:45<01:45,  1.72s/it]\u001b[A\n",
            " 52% 65/125 [01:46<01:44,  1.74s/it]\u001b[A\n",
            " 53% 66/125 [01:48<01:42,  1.74s/it]\u001b[A\n",
            " 54% 67/125 [01:50<01:40,  1.74s/it]\u001b[A\n",
            " 54% 68/125 [01:52<01:39,  1.74s/it]\u001b[A\n",
            " 55% 69/125 [01:53<01:36,  1.73s/it]\u001b[A\n",
            " 56% 70/125 [01:55<01:34,  1.72s/it]\u001b[A\n",
            " 57% 71/125 [01:57<01:32,  1.72s/it]\u001b[A\n",
            " 58% 72/125 [01:58<01:31,  1.72s/it]\u001b[A\n",
            " 58% 73/125 [02:00<01:29,  1.72s/it]\u001b[A\n",
            " 59% 74/125 [02:02<01:27,  1.71s/it]\u001b[A\n",
            " 60% 75/125 [02:04<01:25,  1.71s/it]\u001b[A\n",
            " 61% 76/125 [02:05<01:23,  1.71s/it]\u001b[A\n",
            " 62% 77/125 [02:07<01:20,  1.67s/it]\u001b[A\n",
            " 62% 78/125 [02:08<01:17,  1.65s/it]\u001b[A\n",
            " 63% 79/125 [02:10<01:15,  1.64s/it]\u001b[A\n",
            " 64% 80/125 [02:12<01:13,  1.63s/it]\u001b[A\n",
            " 65% 81/125 [02:13<01:11,  1.63s/it]\u001b[A\n",
            " 66% 82/125 [02:15<01:09,  1.62s/it]\u001b[A\n",
            " 66% 83/125 [02:17<01:07,  1.61s/it]\u001b[A\n",
            " 67% 84/125 [02:18<01:05,  1.61s/it]\u001b[A\n",
            " 68% 85/125 [02:20<01:04,  1.60s/it]\u001b[A\n",
            " 69% 86/125 [02:21<01:02,  1.59s/it]\u001b[A\n",
            " 70% 87/125 [02:23<01:00,  1.58s/it]\u001b[A\n",
            " 70% 88/125 [02:24<00:58,  1.59s/it]\u001b[A\n",
            " 71% 89/125 [02:26<00:57,  1.59s/it]\u001b[A\n",
            " 72% 90/125 [02:28<00:55,  1.58s/it]\u001b[A\n",
            " 73% 91/125 [02:29<00:53,  1.57s/it]\u001b[A\n",
            " 74% 92/125 [02:31<00:51,  1.57s/it]\u001b[A\n",
            " 74% 93/125 [02:32<00:49,  1.56s/it]\u001b[A\n",
            " 75% 94/125 [02:34<00:51,  1.68s/it]\u001b[A\n",
            " 76% 95/125 [02:36<00:51,  1.70s/it]\u001b[A\n",
            " 77% 96/125 [02:38<00:49,  1.72s/it]\u001b[A\n",
            " 78% 97/125 [02:39<00:48,  1.73s/it]\u001b[A\n",
            " 78% 98/125 [02:41<00:46,  1.73s/it]\u001b[A\n",
            " 79% 99/125 [02:43<00:45,  1.73s/it]\u001b[A\n",
            " 80% 100/125 [02:45<00:43,  1.73s/it]\u001b[A\n",
            " 81% 101/125 [02:46<00:41,  1.72s/it]\u001b[A\n",
            " 82% 102/125 [02:48<00:39,  1.72s/it]\u001b[A\n",
            " 82% 103/125 [02:50<00:37,  1.72s/it]\u001b[A\n",
            " 83% 104/125 [02:51<00:36,  1.72s/it]\u001b[A\n",
            " 84% 105/125 [02:53<00:34,  1.72s/it]\u001b[A\n",
            " 85% 106/125 [02:55<00:32,  1.71s/it]\u001b[A\n",
            " 86% 107/125 [02:57<00:30,  1.71s/it]\u001b[A\n",
            " 86% 108/125 [02:58<00:28,  1.66s/it]\u001b[A\n",
            " 87% 109/125 [03:00<00:26,  1.64s/it]\u001b[A\n",
            " 88% 110/125 [03:01<00:24,  1.63s/it]\u001b[A\n",
            " 89% 111/125 [03:03<00:22,  1.62s/it]\u001b[A\n",
            " 90% 112/125 [03:05<00:21,  1.62s/it]\u001b[A\n",
            " 90% 113/125 [03:06<00:19,  1.62s/it]\u001b[A\n",
            " 91% 114/125 [03:08<00:17,  1.61s/it]\u001b[A\n",
            " 92% 115/125 [03:09<00:16,  1.61s/it]\u001b[A\n",
            " 93% 116/125 [03:11<00:14,  1.61s/it]\u001b[A\n",
            " 94% 117/125 [03:13<00:12,  1.59s/it]\u001b[A\n",
            " 94% 118/125 [03:14<00:11,  1.58s/it]\u001b[A\n",
            " 95% 119/125 [03:16<00:09,  1.58s/it]\u001b[A\n",
            " 96% 120/125 [03:17<00:07,  1.58s/it]\u001b[A\n",
            " 97% 121/125 [03:19<00:06,  1.58s/it]\u001b[A\n",
            " 98% 122/125 [03:20<00:04,  1.58s/it]\u001b[A\n",
            " 98% 123/125 [03:22<00:03,  1.56s/it]\u001b[A\n",
            " 99% 124/125 [03:23<00:01,  1.55s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.6225124597549438, 'eval_auc': 0.44998800191969285, 'eval_runtime': 207.7096, 'eval_samples_per_second': 4.814, 'eval_steps_per_second': 0.602, 'epoch': 5.0}\n",
            " 50% 20/40 [15:04<08:53, 26.69s/it]\n",
            "100% 125/125 [03:25<00:00,  1.60s/it]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 1.6703, 'grad_norm': 0.5654699802398682, 'learning_rate': 9e-05, 'epoch': 6.0}\n",
            " 75% 30/40 [19:07<04:29, 26.99s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:01<01:48,  1.13it/s]\u001b[A\n",
            "  2% 3/125 [00:03<02:30,  1.23s/it]\u001b[A\n",
            "  3% 4/125 [00:05<02:51,  1.42s/it]\u001b[A\n",
            "  4% 5/125 [00:06<03:03,  1.53s/it]\u001b[A\n",
            "  5% 6/125 [00:08<03:09,  1.60s/it]\u001b[A\n",
            "  6% 7/125 [00:10<03:13,  1.64s/it]\u001b[A\n",
            "  6% 8/125 [00:12<03:13,  1.66s/it]\u001b[A\n",
            "  7% 9/125 [00:13<03:13,  1.67s/it]\u001b[A\n",
            "  8% 10/125 [00:15<03:13,  1.68s/it]\u001b[A\n",
            "  9% 11/125 [00:17<03:12,  1.69s/it]\u001b[A\n",
            " 10% 12/125 [00:18<03:10,  1.69s/it]\u001b[A\n",
            " 10% 13/125 [00:20<03:09,  1.69s/it]\u001b[A\n",
            " 11% 14/125 [00:22<03:08,  1.70s/it]\u001b[A\n",
            " 12% 15/125 [00:24<03:07,  1.70s/it]\u001b[A\n",
            " 13% 16/125 [00:25<03:06,  1.71s/it]\u001b[A\n",
            " 14% 17/125 [00:27<03:00,  1.68s/it]\u001b[A\n",
            " 14% 18/125 [00:28<02:56,  1.65s/it]\u001b[A\n",
            " 15% 19/125 [00:30<02:53,  1.63s/it]\u001b[A\n",
            " 16% 20/125 [00:32<02:50,  1.62s/it]\u001b[A\n",
            " 17% 21/125 [00:33<02:47,  1.62s/it]\u001b[A\n",
            " 18% 22/125 [00:35<02:45,  1.61s/it]\u001b[A\n",
            " 18% 23/125 [00:36<02:44,  1.61s/it]\u001b[A\n",
            " 19% 24/125 [00:38<02:42,  1.61s/it]\u001b[A\n",
            " 20% 25/125 [00:40<02:40,  1.60s/it]\u001b[A\n",
            " 21% 26/125 [00:41<02:37,  1.59s/it]\u001b[A\n",
            " 22% 27/125 [00:43<02:34,  1.58s/it]\u001b[A\n",
            " 22% 28/125 [00:44<02:32,  1.58s/it]\u001b[A\n",
            " 23% 29/125 [00:46<02:30,  1.57s/it]\u001b[A\n",
            " 24% 30/125 [00:47<02:28,  1.57s/it]\u001b[A\n",
            " 25% 31/125 [00:49<02:26,  1.55s/it]\u001b[A\n",
            " 26% 32/125 [00:51<02:36,  1.68s/it]\u001b[A\n",
            " 26% 33/125 [00:53<02:38,  1.72s/it]\u001b[A\n",
            " 27% 34/125 [00:55<02:38,  1.74s/it]\u001b[A\n",
            " 28% 35/125 [00:56<02:36,  1.74s/it]\u001b[A\n",
            " 29% 36/125 [00:58<02:34,  1.74s/it]\u001b[A\n",
            " 30% 37/125 [01:00<02:32,  1.74s/it]\u001b[A\n",
            " 30% 38/125 [01:01<02:30,  1.73s/it]\u001b[A\n",
            " 31% 39/125 [01:03<02:28,  1.72s/it]\u001b[A\n",
            " 32% 40/125 [01:05<02:26,  1.72s/it]\u001b[A\n",
            " 33% 41/125 [01:07<02:24,  1.72s/it]\u001b[A\n",
            " 34% 42/125 [01:08<02:22,  1.71s/it]\u001b[A\n",
            " 34% 43/125 [01:10<02:20,  1.71s/it]\u001b[A\n",
            " 35% 44/125 [01:12<02:18,  1.71s/it]\u001b[A\n",
            " 36% 45/125 [01:13<02:16,  1.70s/it]\u001b[A\n",
            " 37% 46/125 [01:15<02:14,  1.70s/it]\u001b[A\n",
            " 38% 47/125 [01:17<02:11,  1.68s/it]\u001b[A\n",
            " 38% 48/125 [01:18<02:08,  1.67s/it]\u001b[A\n",
            " 39% 49/125 [01:20<02:05,  1.65s/it]\u001b[A\n",
            " 40% 50/125 [01:22<02:02,  1.63s/it]\u001b[A\n",
            " 41% 51/125 [01:23<01:59,  1.62s/it]\u001b[A\n",
            " 42% 52/125 [01:25<01:57,  1.61s/it]\u001b[A\n",
            " 42% 53/125 [01:26<01:55,  1.61s/it]\u001b[A\n",
            " 43% 54/125 [01:28<01:53,  1.60s/it]\u001b[A\n",
            " 44% 55/125 [01:30<01:52,  1.61s/it]\u001b[A\n",
            " 45% 56/125 [01:31<01:50,  1.60s/it]\u001b[A\n",
            " 46% 57/125 [01:33<01:48,  1.59s/it]\u001b[A\n",
            " 46% 58/125 [01:34<01:46,  1.58s/it]\u001b[A\n",
            " 47% 59/125 [01:36<01:44,  1.58s/it]\u001b[A\n",
            " 48% 60/125 [01:37<01:42,  1.57s/it]\u001b[A\n",
            " 49% 61/125 [01:39<01:39,  1.56s/it]\u001b[A\n",
            " 50% 62/125 [01:40<01:37,  1.54s/it]\u001b[A\n",
            " 50% 63/125 [01:42<01:43,  1.67s/it]\u001b[A\n",
            " 51% 64/125 [01:44<01:47,  1.75s/it]\u001b[A\n",
            " 52% 65/125 [01:46<01:45,  1.77s/it]\u001b[A\n",
            " 53% 66/125 [01:48<01:43,  1.76s/it]\u001b[A\n",
            " 54% 67/125 [01:50<01:41,  1.75s/it]\u001b[A\n",
            " 54% 68/125 [01:51<01:39,  1.75s/it]\u001b[A\n",
            " 55% 69/125 [01:53<01:37,  1.74s/it]\u001b[A\n",
            " 56% 70/125 [01:55<01:35,  1.73s/it]\u001b[A\n",
            " 57% 71/125 [01:57<01:33,  1.73s/it]\u001b[A\n",
            " 58% 72/125 [01:58<01:31,  1.73s/it]\u001b[A\n",
            " 58% 73/125 [02:00<01:29,  1.73s/it]\u001b[A\n",
            " 59% 74/125 [02:02<01:27,  1.72s/it]\u001b[A\n",
            " 60% 75/125 [02:03<01:25,  1.71s/it]\u001b[A\n",
            " 61% 76/125 [02:05<01:23,  1.71s/it]\u001b[A\n",
            " 62% 77/125 [02:07<01:21,  1.71s/it]\u001b[A\n",
            " 62% 78/125 [02:08<01:18,  1.67s/it]\u001b[A\n",
            " 63% 79/125 [02:10<01:16,  1.65s/it]\u001b[A\n",
            " 64% 80/125 [02:12<01:13,  1.64s/it]\u001b[A\n",
            " 65% 81/125 [02:13<01:11,  1.63s/it]\u001b[A\n",
            " 66% 82/125 [02:15<01:09,  1.62s/it]\u001b[A\n",
            " 66% 83/125 [02:16<01:07,  1.61s/it]\u001b[A\n",
            " 67% 84/125 [02:18<01:05,  1.61s/it]\u001b[A\n",
            " 68% 85/125 [02:20<01:04,  1.60s/it]\u001b[A\n",
            " 69% 86/125 [02:21<01:02,  1.59s/it]\u001b[A\n",
            " 70% 87/125 [02:23<01:00,  1.59s/it]\u001b[A\n",
            " 70% 88/125 [02:24<00:58,  1.58s/it]\u001b[A\n",
            " 71% 89/125 [02:26<00:56,  1.58s/it]\u001b[A\n",
            " 72% 90/125 [02:27<00:55,  1.58s/it]\u001b[A\n",
            " 73% 91/125 [02:29<00:53,  1.57s/it]\u001b[A\n",
            " 74% 92/125 [02:30<00:51,  1.55s/it]\u001b[A\n",
            " 74% 93/125 [02:32<00:49,  1.54s/it]\u001b[A\n",
            " 75% 94/125 [02:34<00:51,  1.67s/it]\u001b[A\n",
            " 76% 95/125 [02:36<00:51,  1.71s/it]\u001b[A\n",
            " 77% 96/125 [02:38<00:50,  1.74s/it]\u001b[A\n",
            " 78% 97/125 [02:39<00:49,  1.76s/it]\u001b[A\n",
            " 78% 98/125 [02:41<00:47,  1.75s/it]\u001b[A\n",
            " 79% 99/125 [02:43<00:45,  1.75s/it]\u001b[A\n",
            " 80% 100/125 [02:45<00:43,  1.74s/it]\u001b[A\n",
            " 81% 101/125 [02:46<00:41,  1.73s/it]\u001b[A\n",
            " 82% 102/125 [02:48<00:39,  1.72s/it]\u001b[A\n",
            " 82% 103/125 [02:50<00:37,  1.72s/it]\u001b[A\n",
            " 83% 104/125 [02:51<00:36,  1.72s/it]\u001b[A\n",
            " 84% 105/125 [02:53<00:34,  1.72s/it]\u001b[A\n",
            " 85% 106/125 [02:55<00:32,  1.71s/it]\u001b[A\n",
            " 86% 107/125 [02:57<00:30,  1.71s/it]\u001b[A\n",
            " 86% 108/125 [02:58<00:29,  1.71s/it]\u001b[A\n",
            " 87% 109/125 [03:00<00:27,  1.71s/it]\u001b[A\n",
            " 88% 110/125 [03:02<00:24,  1.66s/it]\u001b[A\n",
            " 89% 111/125 [03:03<00:23,  1.65s/it]\u001b[A\n",
            " 90% 112/125 [03:05<00:21,  1.64s/it]\u001b[A\n",
            " 90% 113/125 [03:06<00:19,  1.63s/it]\u001b[A\n",
            " 91% 114/125 [03:08<00:17,  1.62s/it]\u001b[A\n",
            " 92% 115/125 [03:10<00:16,  1.61s/it]\u001b[A\n",
            " 93% 116/125 [03:11<00:14,  1.61s/it]\u001b[A\n",
            " 94% 117/125 [03:13<00:12,  1.60s/it]\u001b[A\n",
            " 94% 118/125 [03:14<00:11,  1.59s/it]\u001b[A\n",
            " 95% 119/125 [03:16<00:09,  1.59s/it]\u001b[A\n",
            " 96% 120/125 [03:17<00:07,  1.59s/it]\u001b[A\n",
            " 97% 121/125 [03:19<00:06,  1.59s/it]\u001b[A\n",
            " 98% 122/125 [03:21<00:04,  1.58s/it]\u001b[A\n",
            " 98% 123/125 [03:22<00:03,  1.56s/it]\u001b[A\n",
            " 99% 124/125 [03:24<00:01,  1.55s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2864760160446167, 'eval_auc': 0.5007998720204767, 'eval_runtime': 207.9406, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 0.601, 'epoch': 7.5}\n",
            " 75% 30/40 [22:36<04:29, 26.99s/it]\n",
            "100% 125/125 [03:25<00:00,  1.62s/it]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 1.4092, 'grad_norm': 0.7391752600669861, 'learning_rate': 5e-05, 'epoch': 8.0}\n",
            "{'loss': 1.1602, 'grad_norm': 0.8399047255516052, 'learning_rate': 1e-05, 'epoch': 10.0}\n",
            "100% 40/40 [26:39<00:00, 26.74s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:01<01:57,  1.05it/s]\u001b[A\n",
            "  2% 3/125 [00:03<02:37,  1.29s/it]\u001b[A\n",
            "  3% 4/125 [00:05<02:57,  1.47s/it]\u001b[A\n",
            "  4% 5/125 [00:07<03:07,  1.56s/it]\u001b[A\n",
            "  5% 6/125 [00:08<03:12,  1.62s/it]\u001b[A\n",
            "  6% 7/125 [00:10<03:16,  1.67s/it]\u001b[A\n",
            "  6% 8/125 [00:12<03:18,  1.69s/it]\u001b[A\n",
            "  7% 9/125 [00:14<03:16,  1.69s/it]\u001b[A\n",
            "  8% 10/125 [00:15<03:15,  1.70s/it]\u001b[A\n",
            "  9% 11/125 [00:17<03:13,  1.70s/it]\u001b[A\n",
            " 10% 12/125 [00:19<03:11,  1.70s/it]\u001b[A\n",
            " 10% 13/125 [00:20<03:10,  1.70s/it]\u001b[A\n",
            " 11% 14/125 [00:22<03:08,  1.70s/it]\u001b[A\n",
            " 12% 15/125 [00:24<03:08,  1.71s/it]\u001b[A\n",
            " 13% 16/125 [00:25<03:02,  1.68s/it]\u001b[A\n",
            " 14% 17/125 [00:27<02:58,  1.65s/it]\u001b[A\n",
            " 14% 18/125 [00:29<02:55,  1.64s/it]\u001b[A\n",
            " 15% 19/125 [00:30<02:52,  1.62s/it]\u001b[A\n",
            " 16% 20/125 [00:32<02:49,  1.62s/it]\u001b[A\n",
            " 17% 21/125 [00:33<02:47,  1.61s/it]\u001b[A\n",
            " 18% 22/125 [00:35<02:45,  1.61s/it]\u001b[A\n",
            " 18% 23/125 [00:37<02:44,  1.61s/it]\u001b[A\n",
            " 19% 24/125 [00:38<02:41,  1.60s/it]\u001b[A\n",
            " 20% 25/125 [00:40<02:39,  1.59s/it]\u001b[A\n",
            " 21% 26/125 [00:41<02:36,  1.58s/it]\u001b[A\n",
            " 22% 27/125 [00:43<02:34,  1.57s/it]\u001b[A\n",
            " 22% 28/125 [00:44<02:32,  1.57s/it]\u001b[A\n",
            " 23% 29/125 [00:46<02:30,  1.57s/it]\u001b[A\n",
            " 24% 30/125 [00:48<02:28,  1.56s/it]\u001b[A\n",
            " 25% 31/125 [00:49<02:26,  1.56s/it]\u001b[A\n",
            " 26% 32/125 [00:51<02:36,  1.68s/it]\u001b[A\n",
            " 26% 33/125 [00:53<02:37,  1.71s/it]\u001b[A\n",
            " 27% 34/125 [00:55<02:36,  1.72s/it]\u001b[A\n",
            " 28% 35/125 [00:56<02:34,  1.72s/it]\u001b[A\n",
            " 29% 36/125 [00:58<02:33,  1.72s/it]\u001b[A\n",
            " 30% 37/125 [01:00<02:32,  1.73s/it]\u001b[A\n",
            " 30% 38/125 [01:02<02:29,  1.72s/it]\u001b[A\n",
            " 31% 39/125 [01:03<02:28,  1.73s/it]\u001b[A\n",
            " 32% 40/125 [01:05<02:26,  1.72s/it]\u001b[A\n",
            " 33% 41/125 [01:07<02:24,  1.72s/it]\u001b[A\n",
            " 34% 42/125 [01:08<02:22,  1.71s/it]\u001b[A\n",
            " 34% 43/125 [01:10<02:20,  1.71s/it]\u001b[A\n",
            " 35% 44/125 [01:12<02:18,  1.71s/it]\u001b[A\n",
            " 36% 45/125 [01:14<02:16,  1.71s/it]\u001b[A\n",
            " 37% 46/125 [01:15<02:12,  1.68s/it]\u001b[A\n",
            " 38% 47/125 [01:17<02:09,  1.66s/it]\u001b[A\n",
            " 38% 48/125 [01:18<02:06,  1.64s/it]\u001b[A\n",
            " 39% 49/125 [01:20<02:03,  1.63s/it]\u001b[A\n",
            " 40% 50/125 [01:22<02:01,  1.62s/it]\u001b[A\n",
            " 41% 51/125 [01:23<01:59,  1.61s/it]\u001b[A\n",
            " 42% 52/125 [01:25<01:57,  1.61s/it]\u001b[A\n",
            " 42% 53/125 [01:26<01:55,  1.60s/it]\u001b[A\n",
            " 43% 54/125 [01:28<01:53,  1.61s/it]\u001b[A\n",
            " 44% 55/125 [01:30<01:52,  1.60s/it]\u001b[A\n",
            " 45% 56/125 [01:31<01:50,  1.60s/it]\u001b[A\n",
            " 46% 57/125 [01:33<01:47,  1.59s/it]\u001b[A\n",
            " 46% 58/125 [01:34<01:45,  1.58s/it]\u001b[A\n",
            " 47% 59/125 [01:36<01:43,  1.57s/it]\u001b[A\n",
            " 48% 60/125 [01:37<01:41,  1.57s/it]\u001b[A\n",
            " 49% 61/125 [01:39<01:39,  1.55s/it]\u001b[A\n",
            " 50% 62/125 [01:40<01:37,  1.54s/it]\u001b[A\n",
            " 50% 63/125 [01:42<01:43,  1.67s/it]\u001b[A\n",
            " 51% 64/125 [01:44<01:44,  1.72s/it]\u001b[A\n",
            " 52% 65/125 [01:46<01:44,  1.73s/it]\u001b[A\n",
            " 53% 66/125 [01:48<01:42,  1.75s/it]\u001b[A\n",
            " 54% 67/125 [01:49<01:41,  1.74s/it]\u001b[A\n",
            " 54% 68/125 [01:51<01:39,  1.74s/it]\u001b[A\n",
            " 55% 69/125 [01:53<01:37,  1.74s/it]\u001b[A\n",
            " 56% 70/125 [01:55<01:35,  1.73s/it]\u001b[A\n",
            " 57% 71/125 [01:56<01:33,  1.73s/it]\u001b[A\n",
            " 58% 72/125 [01:58<01:31,  1.73s/it]\u001b[A\n",
            " 58% 73/125 [02:00<01:29,  1.72s/it]\u001b[A\n",
            " 59% 74/125 [02:02<01:27,  1.72s/it]\u001b[A\n",
            " 60% 75/125 [02:03<01:25,  1.71s/it]\u001b[A\n",
            " 61% 76/125 [02:05<01:23,  1.71s/it]\u001b[A\n",
            " 62% 77/125 [02:07<01:21,  1.71s/it]\u001b[A\n",
            " 62% 78/125 [02:08<01:20,  1.71s/it]\u001b[A\n",
            " 63% 79/125 [02:10<01:16,  1.67s/it]\u001b[A\n",
            " 64% 80/125 [02:12<01:14,  1.65s/it]\u001b[A\n",
            " 65% 81/125 [02:13<01:11,  1.64s/it]\u001b[A\n",
            " 66% 82/125 [02:15<01:09,  1.63s/it]\u001b[A\n",
            " 66% 83/125 [02:16<01:07,  1.62s/it]\u001b[A\n",
            " 67% 84/125 [02:18<01:06,  1.61s/it]\u001b[A\n",
            " 68% 85/125 [02:20<01:04,  1.61s/it]\u001b[A\n",
            " 69% 86/125 [02:21<01:02,  1.61s/it]\u001b[A\n",
            " 70% 87/125 [02:23<01:01,  1.61s/it]\u001b[A\n",
            " 70% 88/125 [02:24<00:59,  1.61s/it]\u001b[A\n",
            " 71% 89/125 [02:26<00:57,  1.59s/it]\u001b[A\n",
            " 72% 90/125 [02:27<00:55,  1.58s/it]\u001b[A\n",
            " 73% 91/125 [02:29<00:53,  1.58s/it]\u001b[A\n",
            " 74% 92/125 [02:31<00:51,  1.57s/it]\u001b[A\n",
            " 74% 93/125 [02:32<00:49,  1.56s/it]\u001b[A\n",
            " 75% 94/125 [02:34<00:52,  1.68s/it]\u001b[A\n",
            " 76% 95/125 [02:36<00:51,  1.72s/it]\u001b[A\n",
            " 77% 96/125 [02:38<00:50,  1.74s/it]\u001b[A\n",
            " 78% 97/125 [02:39<00:48,  1.74s/it]\u001b[A\n",
            " 78% 98/125 [02:41<00:46,  1.74s/it]\u001b[A\n",
            " 79% 99/125 [02:43<00:45,  1.74s/it]\u001b[A\n",
            " 80% 100/125 [02:45<00:43,  1.74s/it]\u001b[A\n",
            " 81% 101/125 [02:46<00:41,  1.73s/it]\u001b[A\n",
            " 82% 102/125 [02:48<00:39,  1.72s/it]\u001b[A\n",
            " 82% 103/125 [02:50<00:37,  1.72s/it]\u001b[A\n",
            " 83% 104/125 [02:51<00:36,  1.72s/it]\u001b[A\n",
            " 84% 105/125 [02:53<00:34,  1.72s/it]\u001b[A\n",
            " 85% 106/125 [02:55<00:32,  1.71s/it]\u001b[A\n",
            " 86% 107/125 [02:57<00:30,  1.71s/it]\u001b[A\n",
            " 86% 108/125 [02:58<00:28,  1.70s/it]\u001b[A\n",
            " 87% 109/125 [03:00<00:27,  1.70s/it]\u001b[A\n",
            " 88% 110/125 [03:02<00:25,  1.67s/it]\u001b[A\n",
            " 89% 111/125 [03:03<00:23,  1.67s/it]\u001b[A\n",
            " 90% 112/125 [03:05<00:21,  1.65s/it]\u001b[A\n",
            " 90% 113/125 [03:06<00:19,  1.63s/it]\u001b[A\n",
            " 91% 114/125 [03:08<00:17,  1.62s/it]\u001b[A\n",
            " 92% 115/125 [03:10<00:16,  1.61s/it]\u001b[A\n",
            " 93% 116/125 [03:11<00:14,  1.61s/it]\u001b[A\n",
            " 94% 117/125 [03:13<00:12,  1.61s/it]\u001b[A\n",
            " 94% 118/125 [03:14<00:11,  1.60s/it]\u001b[A\n",
            " 95% 119/125 [03:16<00:09,  1.60s/it]\u001b[A\n",
            " 96% 120/125 [03:18<00:07,  1.59s/it]\u001b[A\n",
            " 97% 121/125 [03:19<00:06,  1.58s/it]\u001b[A\n",
            " 98% 122/125 [03:21<00:04,  1.58s/it]\u001b[A\n",
            " 98% 123/125 [03:22<00:03,  1.56s/it]\u001b[A\n",
            " 99% 124/125 [03:24<00:01,  1.55s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0428370237350464, 'eval_auc': 0.5028528768730337, 'eval_runtime': 207.9462, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 0.601, 'epoch': 10.0}\n",
            "100% 40/40 [30:08<00:00, 26.74s/it]\n",
            "100% 125/125 [03:25<00:00,  1.60s/it]\u001b[A\n",
            "{'train_runtime': 1808.9285, 'train_samples_per_second': 0.354, 'train_steps_per_second': 0.022, 'train_loss': 1.605431032180786, 'epoch': 10.0}\n",
            "100% 40/40 [30:08<00:00, 45.22s/it]\n",
            "\n",
            " If there's a warning about missing keys above, please disregard :)\n"
          ]
        }
      ],
      "source": [
        "! ./shell/instruct_7B.sh 0 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import fire\n",
        "import gradio as gr\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "import transformers\n",
        "import json\n",
        "import os\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:  # noqa: E722\n",
        "    pass\n",
        "\n",
        "base_model = 'baffo32/decapoda-research-llama-7B-hf'\n",
        "model_type = 'alpaca-lora-7b'\n",
        "load_8bit = False\n",
        "lora_weights = './alpaca-lora-7B'\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        load_in_8bit=load_8bit,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map={'': 0}\n",
        "    )\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        device_map={\"\": device},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        lora_weights,\n",
        "        device_map={\"\": device},\n",
        "    )\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "# unwind broken decapoda-research config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "if not load_8bit:\n",
        "    model.half()  # seems to fix bugs for some users.\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "########inference#########\n",
        "\n",
        "instructions=[\"Given the user's preference and unpreference, identify whether the user will like the target movie by answering \\\"Yes.\\\" or \\\"No.\\\".\"]\n",
        "inputs=[\"User Preference: \\\"Paris, Texas (1984)\\\", \\\"Rebel Without a Cause (1955)\\\", \\\"Return of the Pink Panther, The (1974)\\\", \\\"Ace Ventura: Pet Detective (1994)\\\", \\\"Magnificent Seven, The (1954)\\\", \\\"Star Trek: The Wrath of Khan (1982)\\\", \\\"Cat People (1982)\\\", \\\"Orlando (1993)\\\", \\\"Dave (1993)\\\"\\nUser Unpreference: \\\"Kalifornia (1993)\\\"\\nWhether the user will like the target movie \\\"Perez Family, The (1995)\\\"?\"]\n",
        "temperature=1\n",
        "top_p=1.0\n",
        "top_k=40\n",
        "num_beams=1\n",
        "max_new_tokens=128\n",
        "batch_size=1\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  # noqa: E501\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "prompt = [generate_prompt(instruction, input) for instruction, input in zip(instructions, inputs)]\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    top_k=top_k,\n",
        "    num_beams=num_beams\n",
        ")\n",
        "with torch.no_grad():\n",
        "    generation_output = model.generate(\n",
        "        **inputs,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        # batch_size=batch_size,\n",
        "    )\n",
        "s = generation_output.sequences\n",
        "scores = generation_output.scores[0].softmax(dim=-1)\n",
        "logits = torch.tensor(scores[:,[8241, 3782]], dtype=torch.float32).softmax(dim=-1)\n",
        "input_ids = inputs[\"input_ids\"].to(device)\n",
        "L = input_ids.shape[1]\n",
        "s = generation_output.sequences\n",
        "output = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
        "output = [_.split('Response:\\n')[-1] for _ in output]\n",
        "\n",
        "print(output[0])\n"
      ],
      "metadata": {
        "id": "oSfvLGdSl4Cw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511,
          "referenced_widgets": [
            "06604f472e14460a96f06f5473e38045",
            "3a10464a4a104b378beafdb52c2e0d0d",
            "7b0ed1c31cc34182a2d2742760f61f9a",
            "4a596179b90b48959b50ef031c19631c",
            "a2ec9d0876b64b3f9009f40c6855a6be",
            "2cec1d23c24c44008aa72760158f56c2",
            "de61c26196f044cdbcf0859bfd3e6a2a",
            "b18fabd732614ed98d880956225443ee",
            "f3d184d3f5cf425ab40f87823273435d",
            "0985c72b77f743e398534bc30ae724ea",
            "87ef9b921034421ba600b26d7592b875"
          ]
        },
        "outputId": "51310b3f-090e-4aff-a6b0-f84091422c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:568: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06604f472e14460a96f06f5473e38045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:372: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  adapters_weights = torch.load(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes.\n",
            "\n",
            "### Output:\n",
            "Yes.\n",
            "\n",
            "### Explanation:\n",
            "The user's preference and unpreference are \"Paris, Texas (1984)\", \"Rebel Without a Cause (1955)\", \"Return of the Pink Panther, The (1974)\", \"Ace Ventura: Pet Detective (1994)\", \"Magnificent Seven, The (1954)\", \"Star Trek: The Wrath of Khan (1982)\", \"Cat People (1982)\", \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f6721614afcc>:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  logits = torch.tensor(scores[:,[8241, 3782]], dtype=torch.float32).softmax(dim=-1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06604f472e14460a96f06f5473e38045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a10464a4a104b378beafdb52c2e0d0d",
              "IPY_MODEL_7b0ed1c31cc34182a2d2742760f61f9a",
              "IPY_MODEL_4a596179b90b48959b50ef031c19631c"
            ],
            "layout": "IPY_MODEL_a2ec9d0876b64b3f9009f40c6855a6be"
          }
        },
        "3a10464a4a104b378beafdb52c2e0d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cec1d23c24c44008aa72760158f56c2",
            "placeholder": "​",
            "style": "IPY_MODEL_de61c26196f044cdbcf0859bfd3e6a2a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7b0ed1c31cc34182a2d2742760f61f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18fabd732614ed98d880956225443ee",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3d184d3f5cf425ab40f87823273435d",
            "value": 33
          }
        },
        "4a596179b90b48959b50ef031c19631c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0985c72b77f743e398534bc30ae724ea",
            "placeholder": "​",
            "style": "IPY_MODEL_87ef9b921034421ba600b26d7592b875",
            "value": " 33/33 [01:07&lt;00:00,  2.10s/it]"
          }
        },
        "a2ec9d0876b64b3f9009f40c6855a6be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cec1d23c24c44008aa72760158f56c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de61c26196f044cdbcf0859bfd3e6a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b18fabd732614ed98d880956225443ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d184d3f5cf425ab40f87823273435d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0985c72b77f743e398534bc30ae724ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ef9b921034421ba600b26d7592b875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}